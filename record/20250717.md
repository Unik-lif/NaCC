## 实验记录
### 数据共享
阅读opensbi的scratch地址区域的实现

首先在coldboot hart中，会对其他的harts所对应的scratch区域都做一个初始化
```C
/** Representation of per-HART scratch space */
// 正好是15个大指针，因此一开始的scratch数据结构有15项，之后则继续往下添加，15这个值被存放到了extra_offset中
struct sbi_scratch {
	/** Start (or base) address of firmware linked to OpenSBI library */
	unsigned long fw_start;
	/** Size (in bytes) of firmware linked to OpenSBI library */
	unsigned long fw_size;
	/** Offset (in bytes) of the R/W section */
	unsigned long fw_rw_offset;
	/** Offset (in bytes) of the heap area */
	unsigned long fw_heap_offset;
	/** Size (in bytes) of the heap area */
	unsigned long fw_heap_size;
	/** Arg1 (or 'a1' register) of next booting stage for this HART */
	unsigned long next_arg1;
	/** Address of next booting stage for this HART */
	unsigned long next_addr;
	/** Privilege mode of next booting stage for this HART */
	unsigned long next_mode;
	/** Warm boot entry point address for this HART */
	// 其他warm核的_start_warm所对应的位置，这个初始化还是boot hart做的
	unsigned long warmboot_addr;
	/** Address of sbi_platform */
	// sbi_platform所对应的地址，里头有平台对应的一系列函数操作等信息
	unsigned long platform_addr;
	/** Address of HART ID to sbi_scratch conversion function */
	unsigned long hartid_to_scratch;
	/** Address of current trap context */
	unsigned long trap_context;
	/** Temporary storage */
	unsigned long tmp0;
	/** Options for OpenSBI library */
	unsigned long options;
	/** Index of the hart */
	unsigned long hartindex;
};
// 这个数据结构中的hartid_to_scratch表示的
int sbi_scratch_init(struct sbi_scratch *scratch)
{
	u32 h, hart_count;
	const struct sbi_platform *plat = sbi_platform_ptr(scratch);

	hart_count = plat->hart_count;
	if (hart_count > SBI_HARTMASK_MAX_BITS)
		hart_count = SBI_HARTMASK_MAX_BITS;
	sbi_scratch_hart_count = hart_count;

	// 这边的hartid2scratch调用的是在fw_base.S中的函数
	// 		.section .entry, "ax", %progbits
	// 	.align 3
	// 	.globl _hartid_to_scratch
	// _hartid_to_scratch:
	// 	/*
	// 	 * a0 -> HART ID (passed by caller)
	// 	 * a1 -> HART Index (passed by caller)
	// 	 * t0 -> HART Stack Size
	// 	 * t1 -> HART Stack End
	// 	 * t2 -> Temporary
	// 	 */
	// 	lla	t2, platform
	// #if __riscv_xlen > 32
	//  // 这边从scratch数据加载进来
	// 	lwu	t0, SBI_PLATFORM_HART_STACK_SIZE_OFFSET(t2)
	// 	lwu	t2, SBI_PLATFORM_HART_COUNT_OFFSET(t2)
	// #else
	// 	lw	t0, SBI_PLATFORM_HART_STACK_SIZE_OFFSET(t2)
	// 	lw	t2, SBI_PLATFORM_HART_COUNT_OFFSET(t2)
	// #endif
	// 	sub	t2, t2, a1
	// 	mul	t2, t2, t0
	// 	lla	t1, _fw_end
	// 	add	t1, t1, t2
	// 	li	t2, SBI_SCRATCH_SIZE
	// 	sub	a0, t1, t2
	// 	ret
	// 设置每个hartid所对应的scratch数据结构的位置，从0x8004e000到0x8004c000, 0x8004a000, 0x80048000，分别对应0，1,2,3号核
	// 每个hartid对应的scratch位置：scratch_address = _fw_end + (hart_count - hart_index) * hart_stack_size - SBI_SCRATCH_SIZE
	// 每个hartid都被赋予了2个页的自由使用空间
	// 我们可以看到最后使用的scratch物理地址，以及栈的物理地址是_fw_end之后的一块长度为hart_num * 0x2000的空间
	// 不同hartid所对应的scratch数据结构将会存放到hartindex_to_scratch_table中存储起来
	sbi_for_each_hartindex(i) {
		h = (plat->hart_index2id) ? plat->hart_index2id[i] : i;
		hartindex_to_hartid_table[i] = h;
		hartindex_to_scratch_table[i] =
			((hartid2scratch)scratch->hartid_to_scratch)(h, i);
	}

	return 0;
}
```
这边的结构有点像这样

```
高地址
┌─────────────────┐
│   Hart 0 Stack  │
├─────────────────┤  
│ Hart 0 Scratch  │ ← hartindex_to_scratch_table[0]
├─────────────────┤
│   Hart 1 Stack  │
├─────────────────┤
│ Hart 1 Scratch  │ ← hartindex_to_scratch_table[1] 
├─────────────────┤
│       ...       │
├─────────────────┤
│ _fw_end         │ ← 固件结束地址
│ (firmware)      │
└─────────────────┘
低地址
```

我们看到对于每个hart，都分配了scratch区域，可以通过hartindex尝试去访问之

在做sbi_domain_init时，首先用sbi_scratch_alloc_offset分配了domain_hart_ptr_offset这个指针
```C
int sbi_domain_init(struct sbi_scratch *scratch, u32 cold_hartid)
{
	int rc;
	struct sbi_hartmask *root_hmask;
	struct sbi_domain_memregion *root_memregs;

	SBI_INIT_LIST_HEAD(&domain_list);

	if (scratch->fw_rw_offset == 0 ||
	    (scratch->fw_rw_offset & (scratch->fw_rw_offset - 1)) != 0) {
		sbi_printf("%s: fw_rw_offset is not a power of 2 (0x%lx)\n",
			   __func__, scratch->fw_rw_offset);
		return SBI_EINVAL;
	}

	if ((scratch->fw_start & (scratch->fw_rw_offset - 1)) != 0) {
		sbi_printf("%s: fw_start and fw_rw_offset not aligned\n",
			   __func__);
		return SBI_EINVAL;
	}

	domain_hart_ptr_offset = sbi_scratch_alloc_type_offset(void *);
	if (!domain_hart_ptr_offset)
		return SBI_ENOMEM;
	// ..........
}
// 这个函数有趣的地方在于，它从scratch的extra_offset开始往下增长的，而且它还做了同步上的分配
unsigned long sbi_scratch_alloc_offset(unsigned long size)
{
	void *ptr;
	unsigned long ret = 0;
	struct sbi_scratch *rscratch;
	unsigned long scratch_alloc_align = 0;

	/*
	 * We have a simple brain-dead allocator which never expects
	 * anything to be free-ed hence it keeps incrementing the
	 * next allocation offset until it runs-out of space.
	 *
	 * In future, we will have more sophisticated allocator which
	 * will allow us to re-claim free-ed space.
	 */

	if (!size)
		return 0;

	scratch_alloc_align = sbi_get_scratch_alloc_align();

	/*
	 * We let the allocation align to cacheline bytes to avoid livelock on
	 * certain platforms due to atomic variables from the same cache line.
	 */
	size += scratch_alloc_align - 1;
	size &= ~(scratch_alloc_align - 1);

	spin_lock(&extra_lock);

	if (SBI_SCRATCH_SIZE < (extra_offset + size))
		goto done;

	ret = extra_offset;
	extra_offset += size;

done:
	spin_unlock(&extra_lock);

	// 保证每一个hartid都被分配了，这就很有意思
	if (ret) {
		sbi_for_each_hartindex(i) {
			// 先找到各自的scratch位置
			rscratch = sbi_hartindex_to_scratch(i);
			if (!rscratch)
				continue;
			// 然后再把指针指到extra_offset的末尾
			// 给这个区域做了初始化
			// 当然，这边只是分配了而已，并没有真的把值写进去
			ptr = sbi_scratch_offset_ptr(rscratch, ret);
			sbi_memset(ptr, 0, size);
		}
	}

	return ret;
}
```
利用这个方式把domain_hart_ptr_offset全部alloc到各自的scratch区域之后，我们看到在sbi_domain_register对于这一部分数据的使用
```C
int sbi_domain_register(struct sbi_domain *dom,
			const struct sbi_hartmask *assign_mask)
{
	/* Assign domain to HART if HART is a possible HART */
	// 先前已经在assign_mask中写清楚了那些核是正在使用的
	sbi_hartmask_for_each_hartindex(i, assign_mask) {
		/* the loop will do -smp times */
		if (!sbi_hartmask_test_hartindex(i, dom->possible_harts))
			continue;
		// 根据hartindex找到scratch位置先，然后把domain_hart_ptr_offset所对应的位置读到
		// 按照逻辑来说，tdom的信息应该是空的，如果有，得清空掉
		tdom = sbi_hartindex_to_domain(i);
		if (tdom)
			sbi_hartmask_clear_hartindex(i,
					&tdom->assigned_harts);
		// 然后就把之前得到的dom信息，写到各自的scratch->domain_hart_ptr位置
		sbi_update_hartindex_to_domain(i, dom);
		// 然后对dom里头也写上，已经覆盖了值的hart有哪些
		sbi_hartmask_set_hartindex(i, &dom->assigned_harts);

		/*
		 * If cold boot HART is assigned to this domain then
		 * override boot HART of this domain.
		 */
		if (sbi_hartindex_to_hartid(i) == cold_hartid &&
		    dom->boot_hartid != cold_hartid) {
			sbi_printf("Domain%d Boot HARTID forced to"
				   " %d\n", dom->index, cold_hartid);
			dom->boot_hartid = cold_hartid;
		}
	}
}
```
domain最终会落实到很多个pmp的配置上，从这里我们注意到opensbi在pmp的更新上，似乎就没有使用过任何同步的方式，在传递数据的时候，似乎直接通过在scratch中集中起来传递值，同时做update就可以了。之后，利用下面的同步方式来实现最终的同步
```C
static void wait_for_coldboot(struct sbi_scratch *scratch)
{
	/* Wait for coldboot to finish */
	while (!__smp_load_acquire(&coldboot_done))
		cpu_relax();
}

static void wake_coldboot_harts(struct sbi_scratch *scratch)
{
	/* Mark coldboot done */
	__smp_store_release(&coldboot_done, 1);
}
```
我们可以使用这边的alloc方式，但是不能使用这边的同步方式
### IPI核间中断同步
```C
int sbi_ipi_init(struct sbi_scratch *scratch, bool cold_boot)
{
	int ret;
	struct sbi_ipi_data *ipi_data;

	if (cold_boot) {
		// 分配了ipi_data区域
		ipi_data_off = sbi_scratch_alloc_offset(sizeof(*ipi_data));
		if (!ipi_data_off)
			return SBI_ENOMEM;
		ret = sbi_ipi_event_create(&ipi_smode_ops);
		if (ret < 0)
			return ret;
		ipi_smode_event = ret;
		ret = sbi_ipi_event_create(&ipi_halt_ops);
		if (ret < 0)
			return ret;
		ipi_halt_event = ret;

		/* Initialize platform IPI support */
		ret = sbi_platform_ipi_init(sbi_platform_ptr(scratch));
		if (ret)
			return ret;
	} else {
		// 如果还没有初始化的话，就会hang住
		if (!ipi_data_off)
			return SBI_ENOMEM;
		if (SBI_IPI_EVENT_MAX <= ipi_smode_event ||
		    SBI_IPI_EVENT_MAX <= ipi_halt_event)
			return SBI_ENOSPC;
	}

	ipi_data = sbi_scratch_offset_ptr(scratch, ipi_data_off);
	ipi_data->ipi_type = 0x00;

	/* Clear any pending IPIs for the current hart */
	sbi_ipi_raw_clear();

	/* Enable software interrupts */
	csr_set(CSR_MIE, MIP_MSIP);

	return 0;
}
```
这边的ipi等函数的同步做的非常高明，

乍一看甚至觉得他是错的

我仔细看了很久，才看出点名堂

```C
	rc = sbi_hsm_init(scratch, true);
	if (rc)
		sbi_hart_hang();

	/*
	 * All non-coldboot HARTs do HSM initialization (i.e. enter HSM state
	 * machine) at the start of the warmboot path so it is wasteful to
	 * have these HARTs busy spin in wait_for_coldboot() until coldboot
	 * path is completed.
	 */
	wake_coldboot_harts(scratch);

```
但实际上同步是从boot hart运行的函数sbi_hsm_init开始的，在这边它把其他核的hart状态hdata都改成了STOPPED，直到自己运行到sbi_domain_startup，才把人家的STOPPED状态改成START_PENDING状态

因此一个warmboot的非boot hart启动的时候，首先会先等boot hart把wake_coldboot_harts前面的部分全部跑完，这里可能还是忙等，所以性能开销很大

为了避免这件事情，在wake_coldboot_harts之前，boot hart在sbi_hsm_init中给了其他harts启动时设置了STOPPED参数
```C
static void __noreturn init_warmboot(struct sbi_scratch *scratch, u32 hartid)
{
	int hstate;

	wait_for_coldboot(scratch);
	// boot hart早已准备了STOPPED状态的hstate给其他核
	hstate = sbi_hsm_hart_get_state(sbi_domain_thishart_ptr(), hartid);
	if (hstate < 0)
		sbi_hart_hang();

	if (hstate == SBI_HSM_STATE_SUSPENDED) {
		init_warm_resume(scratch, hartid);
	} else {
		sbi_ipi_raw_clear();
		init_warm_startup(scratch, hartid);
	}
}

static void __noreturn init_warm_startup(struct sbi_scratch *scratch,
					 u32 hartid)
{
	// wakup忙等唤醒了，往下跑，进入sbi_hsm_init
	int rc;
	unsigned long *count;
	const struct sbi_platform *plat = sbi_platform_ptr(scratch);

	if (!entry_count_offset || !init_count_offset)
		sbi_hart_hang();

	count = sbi_scratch_offset_ptr(scratch, entry_count_offset);
	(*count)++;

	/* Note: This has to be first thing in warmboot init sequence */
	rc = sbi_hsm_init(scratch, false);
	if (rc)
		sbi_hart_hang();
	// -----
}

int sbi_hsm_init(struct sbi_scratch *scratch, bool cold_boot)
{
	struct sbi_scratch *rscratch;
	struct sbi_hsm_data *hdata;

	if (cold_boot) {
		hart_data_offset = sbi_scratch_alloc_offset(sizeof(*hdata));
		if (!hart_data_offset)
			return SBI_ENOMEM;

		/* Initialize hart state data for every hart */
		sbi_for_each_hartindex(i) {
			rscratch = sbi_hartindex_to_scratch(i);
			if (!rscratch)
				continue;

			hdata = sbi_scratch_offset_ptr(rscratch,
						       hart_data_offset);
			ATOMIC_INIT(&hdata->state,
				    (i == current_hartindex()) ?
				    SBI_HSM_STATE_START_PENDING :
				    SBI_HSM_STATE_STOPPED);
			ATOMIC_INIT(&hdata->start_ticket, 0);
		}
	} else {
		// 进入这边，上面的数据结构自然早已经声明好了
		sbi_hsm_hart_wait(scratch);
	}

	return 0;
}

// 进入这边
static void sbi_hsm_hart_wait(struct sbi_scratch *scratch)
{
	unsigned long saved_mie;
	struct sbi_hsm_data *hdata = sbi_scratch_offset_ptr(scratch,
							    hart_data_offset);
	/* Save MIE CSR */
	saved_mie = csr_read(CSR_MIE);

	/* Set MSIE and MEIE bits to receive IPI */
	csr_set(CSR_MIE, MIP_MSIP | MIP_MEIP);

	/* Wait for state transition requested by sbi_hsm_hart_start() */
	// 属于是刚刚忙等被叫醒，马上被boot hart一个锤子砸昏了，进入wfi中，直到这个状态改变
	// 我们直到sbi_domain_startup才让warm hart离开这个循环继续往下跑，结果没跑多久，又得等到sbi_hsm_hart_start_finish去唤醒他了
	while (atomic_read(&hdata->state) != SBI_HSM_STATE_START_PENDING) {
		/*
		 * If the hsm_dev is ready and it support the hotplug, we can
		 * use the hsm stop for more power saving
		 */
		if (hsm_device_has_hart_hotplug()) {
			sbi_revert_entry_count(scratch);
			hsm_device_hart_stop();
		}

		wfi();
	}

	/* Restore MIE CSR */
	csr_write(CSR_MIE, saved_mie);

	/*
	 * No need to clear IPI here because the sbi_ipi_init() will
	 * clear it for current HART.
	 */
}
```
因此核间中断的代码是正确的