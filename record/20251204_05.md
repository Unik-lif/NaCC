## 实验记录
我们得搞清楚为什么会有这个page fault的问题

我们先前是对页表页做了替换，用机密内存来进行替换，这部分内存Linux只能进行只读操作 然后现在很自然，在程序运行的过程中，会出现一些page fault错误，原本对于其他类型的异常，我们是先trap到agent这边，通过agent的系统调用acall通过转发给linux，让linux代理去执行来做的 

但是，对于已经被机密内存替换的页表页，linux失去了对于其用户态区域的内存的直接操作能力，从而使得handle_page_fault这个函数很多时候可能没有办法成功地去处理异常 我的想法是这样，对于出现page fault的情况，agent首先判断当前的这个虚拟地址，是否是自己通过opensbi分配内存来管理，还是由Linux来管理。我们之前已经把一个机密容器进程所对应的地址空间分成Linux所进行管理的内核态区域，和Linux只能进行读取，而没法改的用户态区域，则Agent就不转发给Linux了，直接和Opensbi之间交流，来解决这个page fault问题 不过我担心Agent这边处理页错误的时候，会有VMA上不匹配的问题，请问这边处理页错误的时候，是否有VMA上的改动？我感觉应该是没有的？因为地址空间没有变？


### 解决方案
首先我们得看懂page_fault具体的处理流程，才能确认是否可以避免转发，直接让agent这边进行处理

得到结论，还是得转发
- handle_page_fault涉及内核metadata数据的更新，因此不可以由agent这边另起炉灶
- agent转发了缺页异常之后，应该还是由linux来跑一遍handle_page_fault

在handle_page_fault中主要有两个层次需要做一些简单的改动
- __handle_mm_fault这个函数中似乎涉及了页表页的修改，如果出现了新的页表页的分配，或者新的页表页的修改，这种敏感操作需要我们下陷到opensbi这边去做
- handle_pte_fault这个函数修改了最后一级页表页

需要修改的函数
- pmd_alloc