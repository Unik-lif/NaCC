## 实验记录
我们得搞清楚为什么会有这个page fault的问题

我们先前是对页表页做了替换，用机密内存来进行替换，这部分内存Linux只能进行只读操作 然后现在很自然，在程序运行的过程中，会出现一些page fault错误，原本对于其他类型的异常，我们是先trap到agent这边，通过agent的系统调用acall通过转发给linux，让linux代理去执行来做的 

但是，对于已经被机密内存替换的页表页，linux失去了对于其用户态区域的内存的直接操作能力，从而使得handle_page_fault这个函数很多时候可能没有办法成功地去处理异常 我的想法是这样，对于出现page fault的情况，agent首先判断当前的这个虚拟地址，是否是自己通过opensbi分配内存来管理，还是由Linux来管理。我们之前已经把一个机密容器进程所对应的地址空间分成Linux所进行管理的内核态区域，和Linux只能进行读取，而没法改的用户态区域，则Agent就不转发给Linux了，直接和Opensbi之间交流，来解决这个page fault问题 不过我担心Agent这边处理页错误的时候，会有VMA上不匹配的问题，请问这边处理页错误的时候，是否有VMA上的改动？我感觉应该是没有的？因为地址空间没有变？


### 解决方案
首先我们得看懂page_fault具体的处理流程，才能确认是否可以避免转发，直接让agent这边进行处理

得到结论，还是得转发
- 最重要原因：handle_page_fault涉及内核metadata数据的更新，因此不可以由agent这边另起炉灶
- agent转发了缺页异常之后，应该还是由linux来跑一遍handle_page_fault

在handle_page_fault中主要有两个层次需要做一些简单的改动
- __handle_mm_fault这个函数中似乎涉及了页表页的修改，如果出现了新的页表页的分配，或者新的页表页的修改，这种敏感操作需要我们下陷到opensbi这边去做
- handle_pte_fault这个函数修改了最后一级页表页

需要修改的函数
- pmd_alloc
```C
/*
 * Allocate page middle directory.
 * We've already handled the fast-path in-line.
 */
int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	spinlock_t *ptl;
    // 这个位置得确保是向opensbi来申请一块物理内存
	pmd_t *new = pmd_alloc_one(mm, address);
	if (!new)
		return -ENOMEM;

	ptl = pud_lock(mm, pud);
	if (!pud_present(*pud)) {
		mm_inc_nr_pmds(mm);
		smp_wmb(); /* See comment in pmd_install() */
		pud_populate(mm, pud, new);
	} else {	/* Another has populated it */
		pmd_free(mm, new);
	}
	spin_unlock(ptl);
	return 0;
}
```
- pmd_alloc
    - __pmd_alloc
        - pmd_alloc_one_noprof
            - pagetable_alloc_noprof
核心函数看起来是这个
```C
/**
 * pagetable_alloc - Allocate pagetables
 * @gfp:    GFP flags
 * @order:  desired pagetable order
 *
 * pagetable_alloc allocates memory for page tables as well as a page table
 * descriptor to describe that memory.
 *
 * Return: The ptdesc describing the allocated page tables.
 */
static inline struct ptdesc *pagetable_alloc_noprof(gfp_t gfp, unsigned int order)
{
	struct page *page = alloc_pages_noprof(gfp | __GFP_COMP, order);

	return page_ptdesc(page);
}

static inline struct page *alloc_pages_noprof(gfp_t gfp_mask, unsigned int order)
{
	return alloc_pages_node_noprof(numa_node_id(), gfp_mask, order);
}

/*
 * Allocate pages, preferring the node given as nid. When nid == NUMA_NO_NODE,
 * prefer the current CPU's closest node. Otherwise node must be valid and
 * online.
 */
static inline struct page *alloc_pages_node_noprof(int nid, gfp_t gfp_mask,
						   unsigned int order)
{
	if (nid == NUMA_NO_NODE)
		nid = numa_mem_id();

	return __alloc_pages_node_noprof(nid, gfp_mask, order);
}

/*
 * Allocate pages, preferring the node given as nid. The node must be valid and
 * online. For more general interface, see alloc_pages_node().
 */
static inline struct page *
__alloc_pages_node_noprof(int nid, gfp_t gfp_mask, unsigned int order)
{
	VM_BUG_ON(nid < 0 || nid >= MAX_NUMNODES);
	warn_if_node_offline(nid, gfp_mask);

	return __alloc_pages_noprof(gfp_mask, order, nid, NULL);
}

/*
 * This is the 'heart' of the zoned buddy allocator.
 */
struct page *__alloc_pages_noprof(gfp_t gfp, unsigned int order,
				      int preferred_nid, nodemask_t *nodemask)
{
	struct page *page;
	unsigned int alloc_flags = ALLOC_WMARK_LOW;
	gfp_t alloc_gfp; /* The gfp_t that was actually used for allocation */
	struct alloc_context ac = { };

	/*
	 * There are several places where we assume that the order value is sane
	 * so bail out early if the request is out of bound.
	 */
	if (WARN_ON_ONCE_GFP(order > MAX_PAGE_ORDER, gfp))
		return NULL;

	gfp &= gfp_allowed_mask;
	/*
	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
	 * resp. GFP_NOIO which has to be inherited for all allocation requests
	 * from a particular context which has been marked by
	 * memalloc_no{fs,io}_{save,restore}. And PF_MEMALLOC_PIN which ensures
	 * movable zones are not used during allocation.
	 */
	gfp = current_gfp_context(gfp);
	alloc_gfp = gfp;
	if (!prepare_alloc_pages(gfp, order, preferred_nid, nodemask, &ac,
			&alloc_gfp, &alloc_flags))
		return NULL;

	/*
	 * Forbid the first pass from falling back to types that fragment
	 * memory until all local zones are considered.
	 */
	alloc_flags |= alloc_flags_nofragment(zonelist_zone(ac.preferred_zoneref), gfp);

	/* First allocation attempt */
	page = get_page_from_freelist(alloc_gfp, order, alloc_flags, &ac);
	if (likely(page))
		goto out;

	alloc_gfp = gfp;
	ac.spread_dirty_pages = false;

	/*
	 * Restore the original nodemask if it was potentially replaced with
	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
	 */
	ac.nodemask = nodemask;

	page = __alloc_pages_slowpath(alloc_gfp, order, &ac);

out:
	if (memcg_kmem_online() && (gfp & __GFP_ACCOUNT) && page &&
	    unlikely(__memcg_kmem_charge_page(page, gfp, order) != 0)) {
		__free_pages(page, order);
		page = NULL;
	}

	trace_mm_page_alloc(page, order, alloc_gfp, ac.migratetype);
	kmsan_alloc_page(page, order, alloc_gfp);

	return page;
}
```
但这样看起来还是很复杂，主要是好像还是依赖struct page*这个数据结构，而目前的它似乎不大能存在，可能还需要更加谨慎地看看