## 实验记录
感觉这边或许还真可以尝试从新的页表这边进行访问，我们之前的方案不见得是最优解？
```C
static unsigned long zap_pte_range(struct mmu_gather *tlb,
				struct vm_area_struct *vma, pmd_t *pmd,
				unsigned long addr, unsigned long end,
				struct zap_details *details)
{
	bool force_flush = false, force_break = false;
	struct mm_struct *mm = tlb->mm;
	int rss[NR_MM_COUNTERS];
	spinlock_t *ptl;
	pte_t *start_pte;
	pte_t *pte;
	swp_entry_t entry;
	int nr;

	tlb_change_page_size(tlb, PAGE_SIZE);
	init_rss_vec(rss);
    // 这边让其返回真实的pte，也就是页表页遍历的那一个
    // 1. 来自全新分配：伪造一个struct page*数据结构，并且对其进行加锁
    // 2. 来自页表页的搬运：找到老的页对应的strcut page*，以进行上锁
    // 但最后返回页表中真实的PTE
    start_pte = pte = pte_offset_map_lock(mm, pmd, addr, &ptl);

	if (!pte)
		return addr;

	flush_tlb_batched_pending(mm);
	arch_enter_lazy_mmu_mode();
	do {
		pte_t ptent = ptep_get(pte);
		struct folio *folio;
		struct page *page;
		int max_nr;

		nr = 1;
		if (pte_none(ptent))
			continue;

		if (need_resched())
			break;
        // 在这里找到对应的这个ptent，他是根据pte的遍历来做的
        // 对于含有present的页表项，我们这边可以再做一个细分，利用reserved bit
        // 1. 如果是old pte，也就是搬运PTP后，尚且没有被改动过的页表项，那么直接走正常路径就好，需要注意是否有涉及老页表页信息的函数操作，如果涉及，需要做好映射和修改
        // 2. 如果是new pte，也就是搬运PTP后新增加的页表项，或者出现改动的页表项（这两种情况是否相同？），从老的页表页这边看没有参考价值，不过这些页表项也是正常通过VMA和linux分配所产生的，因此其他步骤应该是一样的，就是类似zap_present_ptes这一步可能需要有所不同？
        // 似乎没有任何不同，因为同样是VMA来进行分配的，唯一不同仅是在页表页这边出现了区别，不过我们可以为了提高性能，选择不把new pte entry清零，这样就避免了一次下陷的过程
		if (pte_present(ptent)) {
			max_nr = (end - addr) / PAGE_SIZE;
			nr = zap_present_ptes(tlb, vma, pte, ptent, max_nr,
					      addr, details, rss, &force_flush,
					      &force_break);
			if (unlikely(force_break)) {
				addr += nr * PAGE_SIZE;
				break;
			}
			continue;
		}

		entry = pte_to_swp_entry(ptent);
		if (is_device_private_entry(entry) ||
		    is_device_exclusive_entry(entry)) {
			page = pfn_swap_entry_to_page(entry);
			folio = page_folio(page);
			if (unlikely(!should_zap_folio(details, folio)))
				continue;
			/*
			 * Both device private/exclusive mappings should only
			 * work with anonymous page so far, so we don't need to
			 * consider uffd-wp bit when zap. For more information,
			 * see zap_install_uffd_wp_if_needed().
			 */
			WARN_ON_ONCE(!vma_is_anonymous(vma));
			rss[mm_counter(folio)]--;
			if (is_device_private_entry(entry))
				folio_remove_rmap_pte(folio, page, vma);
			folio_put(folio);
		} else if (!non_swap_entry(entry)) {
			max_nr = (end - addr) / PAGE_SIZE;
			nr = swap_pte_batch(pte, max_nr, ptent);
			/* Genuine swap entries, hence a private anon pages */
			if (!should_zap_cows(details))
				continue;
			rss[MM_SWAPENTS] -= nr;
			free_swap_and_cache_nr(entry, nr);
		} else if (is_migration_entry(entry)) {
			folio = pfn_swap_entry_folio(entry);
			if (!should_zap_folio(details, folio))
				continue;
			rss[mm_counter(folio)]--;
		} else if (pte_marker_entry_uffd_wp(entry)) {
			/*
			 * For anon: always drop the marker; for file: only
			 * drop the marker if explicitly requested.
			 */
			if (!vma_is_anonymous(vma) &&
			    !zap_drop_file_uffd_wp(details))
				continue;
		} else if (is_hwpoison_entry(entry) ||
			   is_poisoned_swp_entry(entry)) {
			if (!should_zap_cows(details))
				continue;
		} else {
			/* We should have covered all the swap entry types */
			pr_alert("unrecognized swap entry 0x%lx\n", entry.val);
			WARN_ON_ONCE(1);
		}
		clear_not_present_full_ptes(mm, addr, pte, nr, tlb->fullmm);
		zap_install_uffd_wp_if_needed(vma, addr, pte, nr, details, ptent);
	} while (pte += nr, addr += PAGE_SIZE * nr, addr != end);

	add_mm_rss_vec(mm, rss);
	arch_leave_lazy_mmu_mode();

	/* Do the actual TLB flush before dropping ptl */
	if (force_flush) {
		tlb_flush_mmu_tlbonly(tlb);
		tlb_flush_rmaps(tlb, vma);
	}
	pte_unmap_unlock(start_pte, ptl);

	/*
	 * If we forced a TLB flush (either due to running out of
	 * batch buffers or because we needed to flush dirty TLB
	 * entries before releasing the ptl), free the batched
	 * memory too. Come back again if we didn't do everything.
	 */
	if (force_flush)
		tlb_flush_mmu(tlb);

	return addr;
}
```
从这里我们似乎可以判断，如果某个末级页表页它是有对应的原本的页表页的，按照这种方式去释放，那么可能还真的能work，可以释放干净真实的物理页。其他场景我们暂时没有遇到，因此我们可以暂时绕过他们，等到真跑测试用例的时候我们再尝试去看

现在对物理页确实做了释放，接下来我们需要释放掉页表页，核心逻辑在这里：
```C
/*
 * Note: this doesn't free the actual pages themselves. That
 * has been handled earlier when unmapping all the memory regions.
 */
static void free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,
			   unsigned long addr)
{
	pgtable_t token;
	if(current->thread.nacc_flag & NACC_RECLAIM){
        // return the old pfn (pte) struct page.
		token = pmd_pgtable_nacc(*pmd);
        // set the old pfn (pmd) entry to zero
        // and recliam the new pfn (pte)
		pmd_clear_nacc(pmd);
		printk(KERN_ERR "free_pte_range: pmd: %lx pmd val: %lx pmd val pfn: %lx\n", (unsigned long)pmd, pmd_val(*pmd), __page_val_to_pfn(pmd_val(*pmd)));
	} else {
		token = pmd_pgtable(*pmd);
		pmd_clear(pmd);
	}
	pte_free_tlb(tlb, token, addr);
	mm_dec_nr_ptes(tlb->mm);
}
```
这里是原本我们释放最底层页表页的逻辑
- 首先得到老的pte页对应的struct page数据，之后会在pte_free_tlb中进行释放
- 然后修改指向老的pte页对应的老的pmdp指针，清空之，同时把真正的pte页放入列表中，后续会发送给opensbi进行清空

现在我们可能需要对其进行改动
- 以真实的页表遍历作为铁证和最基本的原则信息
- pte_free_tlb只能去清空旧的页
    - 我们通过页表遍历得到了pmd之后，运行pmd_pgtable
        - 该项含有NEW标志，这个我们暂时没有遇到，可以暂时不管
        - 该项不含NEW标志，迁移时写上的项，下一级PTE页没有struct page*，拿去尝试清空旧页
- pmd_clear....

好像到我们目前的这种情况(hello world)，这个并不需要做修改

先尝试落实一下末级的这种释放模式